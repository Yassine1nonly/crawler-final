import requests
from bs4 import BeautifulSoup
import pymongo
from datetime import datetime, timedelta
import schedule
import time
import pdfplumber
import io
from typing import List, Dict
import threading
import logging
from urllib.parse import urljoin, urlparse, parse_qs, urlencode
import random
import hashlib
import json
import re
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from collections import defaultdict

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class AdvancedAntiBlockingStrategy:
    """StratÃ©gies anti-blocage avancÃ©es pour le crawling"""
    
    # User-Agents rÃ©alistes et diversifiÃ©s
    USER_AGENTS = [
        # Chrome Windows
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        # Chrome Mac
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        # Firefox
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0',
        # Safari
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
        # Edge
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0',
        # Linux
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    ]
    
    # Referers rÃ©alistes
    REFERERS = [
        'https://www.google.com/',
        'https://www.google.com/search?q=',
        'https://www.bing.com/',
        'https://duckduckgo.com/',
        'https://www.yahoo.com/',
        '',  # Pas de referer parfois
    ]
    
    # Langues communes
    LANGUAGES = [
        'en-US,en;q=0.9',
        'en-GB,en;q=0.9',
        'fr-FR,fr;q=0.9,en;q=0.8',
        'es-ES,es;q=0.9,en;q=0.8',
        'de-DE,de;q=0.9,en;q=0.8',
    ]
    
    PROXIES = []
    
    def __init__(self):
        self.session_fingerprint = self._generate_fingerprint()
        self.cookies_store = {}
    
    @staticmethod
    def _generate_fingerprint():
        """GÃ©nÃ¨re une empreinte unique pour la session"""
        return hashlib.md5(str(time.time()).encode()).hexdigest()[:16]
    
    @staticmethod
    def get_random_user_agent():
        """Retourne un User-Agent alÃ©atoire"""
        return random.choice(AdvancedAntiBlockingStrategy.USER_AGENTS)
    
    @staticmethod
    def get_random_proxy():
        """Retourne un proxy alÃ©atoire"""
        if AdvancedAntiBlockingStrategy.PROXIES:
            return random.choice(AdvancedAntiBlockingStrategy.PROXIES)
        return None
    
    def get_advanced_headers(self, url=None, referer=None):
        """GÃ©nÃ¨re des headers avancÃ©s et rÃ©alistes"""
        headers = {
            'User-Agent': self.get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': random.choice(self.LANGUAGES),
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none' if not referer else 'same-origin',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
        }
        
        # Ajouter referer de maniÃ¨re intelligente
        if referer:
            headers['Referer'] = referer
        elif url and random.random() > 0.3:  # 70% du temps, ajouter un referer
            ref = random.choice(self.REFERERS)
            if 'search?q=' in ref:
                # Simuler une recherche Google
                domain = urlparse(url).netloc
                headers['Referer'] = f"{ref}{domain.replace('www.', '')}"
            else:
                headers['Referer'] = ref
        
        return headers
    
    @staticmethod
    def calculate_intelligent_delay(base_delay=2, domain=None, is_retry=False):
        """Calcule un dÃ©lai intelligent basÃ© sur le contexte"""
        if is_retry:
            # DÃ©lai plus long en cas de retry (rÃ©duit)
            return base_delay * 1.5 + random.uniform(0.2, 0.8)
        
        # Variation naturelle humaine
        human_variance = random.uniform(-0.1, 0.3)
        
        # Ajout de patterns humains (parfois trÃ¨s rapide, parfois lent)
        if random.random() < 0.2:  # 20% du temps, trÃ¨s rapide
            return base_delay * 0.3 + human_variance
        elif random.random() < 0.08:  # 8% du temps, lent
            return base_delay * 1.3 + human_variance
        
        return base_delay + human_variance
    
    def create_advanced_session(self, use_proxy=False, verify_ssl=True):
        """CrÃ©e une session avec configuration avancÃ©e"""
        session = requests.Session()
        session.trust_env = False  # Ignore system proxy env (can break crawling)
        
        # DÃ©sactiver warnings SSL si nÃ©cessaire
        if not verify_ssl:
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # Configuration retry sophistiquÃ©e
        retry = Retry(
            total=5,
            backoff_factor=2,
            status_forcelist=[429, 500, 502, 503, 504, 520, 522, 524],
            allowed_methods=["HEAD", "GET", "OPTIONS", "POST"],
            raise_on_status=False  # Ne pas lever d'exception
        )
        
        adapter = HTTPAdapter(
            max_retries=retry,
            pool_connections=20,
            pool_maxsize=50,
            pool_block=False
        )
        
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        
        # Proxy si disponible
        if use_proxy:
            proxy = self.get_random_proxy()
            if proxy:
                session.proxies.update(proxy)
        
        session.verify = verify_ssl
        
        # Garder les cookies entre requÃªtes (comportement navigateur)
        session.cookies.update(self.cookies_store.get('default', {}))
        
        return session
    
    def save_cookies(self, session, domain='default'):
        """Sauvegarde les cookies pour rÃ©utilisation"""
        self.cookies_store[domain] = session.cookies.get_dict()
    
    @staticmethod
    def normalize_url(url):
        """Normalise une URL pour Ã©viter les doublons"""
        parsed = urlparse(url)
        path = parsed.path or "/"
        
        # Enlever le fragment (#)
        url_without_fragment = url.split('#')[0]
        
        # Trier les paramÃ¨tres de query pour cohÃ©rence
        if parsed.query:
            params = parse_qs(parsed.query)
            sorted_params = sorted(params.items())
            normalized_query = urlencode(sorted_params, doseq=True)
            normalized = f"{parsed.scheme}://{parsed.netloc}{path}?{normalized_query}"
        else:
            normalized = f"{parsed.scheme}://{parsed.netloc}{path}"
        
        # Enlever le trailing slash sauf pour la racine
        if normalized.endswith('/') and parsed.path != '/':
            normalized = normalized[:-1]
        
        return normalized


class AdaptiveRateLimiter:
    """Rate limiter adaptatif qui apprend des rÃ©ponses du serveur"""
    
    def __init__(self):
        self.domain_timers = {}
        self.domain_delays = defaultdict(lambda: 0.2)  # DÃ©lai initial agressif
        self.domain_429_count = defaultdict(int)
        self.lock = threading.Lock()
    
    def wait_if_needed(self, domain, base_delay=2):
        """Attend avec dÃ©lai adaptatif"""
        with self.lock:
            current_time = time.time()
            
            # RÃ©cupÃ©rer le dÃ©lai adaptatif pour ce domaine
            adaptive_delay = self.domain_delays[domain]
            
            if domain in self.domain_timers:
                elapsed = current_time - self.domain_timers[domain]
                if elapsed < adaptive_delay:
                    sleep_time = adaptive_delay - elapsed
                    logger.debug(f"Rate limiting {domain}: {sleep_time:.2f}s")
                    time.sleep(sleep_time)
            
            self.domain_timers[domain] = time.time()
    
    def report_429(self, domain):
        """Signale un rate limit et augmente le dÃ©lai"""
        with self.lock:
            self.domain_429_count[domain] += 1
            # Augmenter progressivement le dÃ©lai
            self.domain_delays[domain] = min(
                self.domain_delays[domain] * 1.5,
                30.0  # Max 30 secondes
            )
            logger.warning(f"Rate limit dÃ©tectÃ© pour {domain}. Nouveau dÃ©lai: {self.domain_delays[domain]:.1f}s")
    
    def report_success(self, domain):
        """Signale un succÃ¨s et rÃ©duit lÃ©gÃ¨rement le dÃ©lai"""
        with self.lock:
            if self.domain_delays[domain] > 0.2:
                self.domain_delays[domain] = max(
                    self.domain_delays[domain] * 0.95,
                    0.2  # Min 0.2 seconde
                )


class JavaScriptChallengeSolver:
    """DÃ©tecte et tente de rÃ©soudre les challenges JavaScript simples"""
    
    @staticmethod
    def detect_challenge(response):
        """DÃ©tecte si la rÃ©ponse contient un challenge JS"""
        indicators = [
            'cloudflare',
            'checking your browser',
            'enable javascript',
            'ddos protection',
            'security check',
            'captcha',
        ]
        
        content_lower = response.text.lower()
        return any(indicator in content_lower for indicator in indicators)
    
    @staticmethod
    def suggest_solutions():
        """SuggÃ¨re des solutions pour contourner les challenges"""
        return [
            "ğŸ’¡ Ce site utilise une protection anti-bot avancÃ©e (Cloudflare/similar)",
            "Solutions possibles:",
            "  1. Utiliser Selenium/Playwright avec un vrai navigateur",
            "  2. Utiliser des services de rÃ©solution CAPTCHA",
            "  3. Utiliser l'API officielle du site",
            "  4. Utiliser des proxies rÃ©sidentiels premium",
        ]


class WebCrawler:
    """Crawler web avec stratÃ©gies anti-blocage avancÃ©es"""
    
    def __init__(self, mongo_uri="mongodb://localhost:27017/", 
                 db_name="web_crawler_db",
                 use_proxy=False,
                 base_delay=0.2,
                 respect_robots_txt=False,
                 verify_ssl=True,
                 max_retries_per_url=2,
                 request_timeout=12,
                 use_browser_fallback=True,
                 mongo_timeout_ms=2000):
        """Initialise le crawler"""
        try:
            self.mongo_available = False
            self.client = pymongo.MongoClient(mongo_uri, serverSelectionTimeoutMS=mongo_timeout_ms)
            try:
                self.client.admin.command('ping')
                self.mongo_available = True
            except Exception:
                logger.warning("âš ï¸ MongoDB indisponible, mode sans stockage")
            
            self.db = self.client[db_name] if self.mongo_available else None
            self.sources_collection = self.db['sources'] if self.mongo_available else None
            self.data_collection = self.db['crawled_data'] if self.mongo_available else None
            self.robots_cache = self.db['robots_cache'] if self.mongo_available else None
            self.url_history = self.db['url_history'] if self.mongo_available else None
            
            if self.mongo_available:
                # Index - avec gestion complÃ¨te des conflits
                try:
                    self.data_collection.create_index([('title', 'text'), ('content', 'text')])
                except:
                    pass
                
                try:
                    self.data_collection.create_index('source_id')
                except:
                    pass
                
                try:
                    self.data_collection.create_index('timestamp')
                except:
                    pass
                
                # Gestion intelligente de l'index URL
                try:
                    # VÃ©rifier si l'index existe dÃ©jÃ 
                    existing_indexes = self.data_collection.index_information()
                    if 'url_1' in existing_indexes:
                        # Si l'index existe sans unique, le supprimer et recrÃ©er
                        current_index = existing_indexes['url_1']
                        if not current_index.get('unique', False):
                            logger.info("ğŸ”„ RecrÃ©ation de l'index URL avec contrainte unique...")
                            self.data_collection.drop_index('url_1')
                            self.data_collection.create_index('url', unique=True, sparse=True, name='url_unique_idx')
                        # Sinon l'index existe dÃ©jÃ  correctement
                    else:
                        # CrÃ©er l'index
                        self.data_collection.create_index('url', unique=True, sparse=True, name='url_unique_idx')
                except pymongo.errors.DuplicateKeyError:
                    logger.warning("âš ï¸  Doublons dÃ©tectÃ©s, index URL sans contrainte unique")
                    try:
                        self.data_collection.drop_index('url_1')
                    except:
                        pass
                    try:
                        self.data_collection.drop_index('url_unique_idx')
                    except:
                        pass
                    self.data_collection.create_index('url', name='url_idx')
                except Exception as e:
                    logger.warning(f"âš ï¸  Index URL non crÃ©Ã©: {e}")
                
                # Index pour url_history
                try:
                    existing_history_indexes = self.url_history.index_information()
                    if 'url_1' not in existing_history_indexes:
                        self.url_history.create_index('url', unique=True, sparse=True)
                except:
                    pass
                
                try:
                    self.url_history.create_index('last_crawled')
                except:
                    pass
            
            # Configuration
            self.use_proxy = use_proxy
            self.base_delay = base_delay
            self.respect_robots_txt = respect_robots_txt
            self.verify_ssl = verify_ssl
            self.max_retries_per_url = max_retries_per_url
            self.request_timeout = request_timeout
            self.use_browser_fallback = use_browser_fallback
            
            # StratÃ©gies anti-blocage
            self.rate_limiter = AdaptiveRateLimiter()
            self.anti_blocking = AdvancedAntiBlockingStrategy()
            self.js_solver = JavaScriptChallengeSolver()
            
            if self.mongo_available:
                logger.info(f"âœ“ MongoDB: {db_name}")
            logger.info(f"âœ“ Config: proxy={use_proxy}, delay={base_delay}s, SSL={verify_ssl}")
            logger.info(f"âœ“ StratÃ©gies avancÃ©es activÃ©es")
        except Exception as e:
            logger.error(f"Erreur MongoDB: {e}")
            raise

    @staticmethod
    def _expand_keywords(keywords):
        expanded = set(keywords)
        for kw in list(expanded):
            if kw == "education":
                expanded.update([
                    "educational",
                    "school",
                    "schools",
                    "student",
                    "students",
                    "teacher",
                    "teachers",
                    "university",
                    "universities",
                    "college",
                    "campus",
                    "classroom",
                    "curriculum",
                    "exam",
                    "exams",
                    "scholarship",
                    "education ministry",
                    "ministry of education",
                    "education system",
                    "enseignement",
                    "ecole",
                    "ecoles",
                    "universite",
                    "universites",
                    "etudiant",
                    "etudiants",
                    "professeur",
                    "professeurs",
                    "formation",
                    "scolarite",
                    "lycee",
                    "bac",
                    "baccalaureat",
                    "Ø§Ù„ØªØ¹Ù„ÙŠÙ…",
                    "Ù…Ø¯Ø±Ø³Ø©",
                    "Ù…Ø¯Ø§Ø±Ø³",
                    "Ø¬Ø§Ù…Ø¹Ø©",
                    "Ø¬Ø§Ù…Ø¹Ø§Øª",
                    "Ø·Ø§Ù„Ø¨",
                    "Ø·Ù„Ø§Ø¨",
                    "ØªÙ„Ù…ÙŠØ°",
                    "ØªÙ„Ø§Ù…ÙŠØ°",
                    "Ø£Ø³ØªØ§Ø°",
                    "Ø£Ø³Ø§ØªØ°Ø©",
                    "Ø§Ù…ØªØ­Ø§Ù†",
                    "Ø§Ù…ØªØ­Ø§Ù†Ø§Øª",
                    "ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ±Ø¨ÙŠØ©",
                    "Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¹Ø§Ù„ÙŠ",
                ])
            elif kw == "finance":
                expanded.update([
                    "financial",
                    "economy",
                    "economic",
                    "bank",
                    "banks",
                    "banking",
                    "investment",
                    "investments",
                    "stock",
                    "stocks",
                    "market",
                    "markets",
                    "bond",
                    "bonds",
                    "inflation",
                    "budget",
                    "tax",
                    "taxes",
                    "loan",
                    "loans",
                    "credit",
                    "currency",
                    "currencies",
                    "fund",
                    "funds",
                    "finance ministry",
                    "ministry of finance",
                    "Ã©conomie",
                    "Ã©conomique",
                    "banque",
                    "banques",
                    "bourse",
                    "marchÃ©",
                    "marchÃ©s",
                    "investissement",
                    "investissements",
                    "inflation",
                    "budget",
                    "impÃ´t",
                    "impÃ´ts",
                    "crÃ©dit",
                    "monnaie",
                    "finances",
                    "ØªÙ…ÙˆÙŠÙ„",
                    "Ù…Ø§Ù„ÙŠ",
                    "Ù…Ø§Ù„ÙŠØ©",
                    "Ø§Ù‚ØªØµØ§Ø¯",
                    "Ø§Ù‚ØªØµØ§Ø¯ÙŠ",
                    "Ø¨Ù†Ùƒ",
                    "Ø¨Ù†ÙˆÙƒ",
                    "Ø§Ø³ØªØ«Ù…Ø§Ø±",
                    "Ø§Ø³ØªØ«Ù…Ø§Ø±Ø§Øª",
                    "Ø¨ÙˆØ±ØµØ©",
                    "Ø³ÙˆÙ‚",
                    "Ø£Ø³ÙˆØ§Ù‚",
                    "ØªØ¶Ø®Ù…",
                    "Ù…ÙŠØ²Ø§Ù†ÙŠØ©",
                    "Ø¶Ø±Ø§Ø¦Ø¨",
                    "Ù‚Ø±Ø¶",
                    "Ù‚Ø±ÙˆØ¶",
                    "ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©",
                ])
            elif kw == "health":
                expanded.update([
                    "healthcare",
                    "medical",
                    "medicine",
                    "doctor",
                    "doctors",
                    "hospital",
                    "hospitals",
                    "clinic",
                    "clinics",
                    "patient",
                    "patients",
                    "public health",
                    "vaccine",
                    "vaccines",
                    "epidemic",
                    "pandemic",
                    "disease",
                    "diseases",
                    "treatment",
                    "pharmacy",
                    "pharmacies",
                    "ministry of health",
                    "santÃ©",
                    "sanitaire",
                    "mÃ©dical",
                    "mÃ©decine",
                    "hÃ´pital",
                    "hÃ´pitaux",
                    "clinique",
                    "cliniques",
                    "patient",
                    "patients",
                    "vaccin",
                    "vaccins",
                    "Ã©pidÃ©mie",
                    "pandÃ©mie",
                    "maladie",
                    "maladies",
                    "traitement",
                    "pharmacie",
                    "pharmacies",
                    "ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø©",
                    "ØµØ­Ø©",
                    "ØµØ­ÙŠ",
                    "Ø·Ø¨ÙŠØ¨",
                    "Ø£Ø·Ø¨Ø§Ø¡",
                    "Ù…Ø³ØªØ´ÙÙ‰",
                    "Ù…Ø³ØªØ´ÙÙŠØ§Øª",
                    "Ø¹ÙŠØ§Ø¯Ø©",
                    "Ø¹ÙŠØ§Ø¯Ø§Øª",
                    "Ù…Ø±ÙŠØ¶",
                    "Ù…Ø±Ø¶Ù‰",
                    "Ù„Ù‚Ø§Ø­",
                    "Ù„Ù‚Ø§Ø­Ø§Øª",
                    "ÙˆØ¨Ø§Ø¡",
                    "Ø¬Ø§Ø¦Ø­Ø©",
                    "Ù…Ø±Ø¶",
                    "Ø£Ù…Ø±Ø§Ø¶",
                    "Ø¹Ù„Ø§Ø¬",
                    "ØµÙŠØ¯Ù„ÙŠØ©",
                    "ØµÙŠØ¯Ù„ÙŠØ§Øª",
                ])
        return list(expanded)

    @staticmethod
    def _normalize_text(text):
        text = (text or "").lower()
        text = WebCrawler._normalize_arabic(text)
        return " ".join(text.split())

    @staticmethod
    def _normalize_arabic(text):
        if not text:
            return ""
        # Normalize common Arabic letter variants and strip diacritics
        replacements = {
            "Ø£": "Ø§",
            "Ø¥": "Ø§",
            "Ø¢": "Ø§",
            "Ù‰": "ÙŠ",
            "Ø¤": "Ùˆ",
            "Ø¦": "ÙŠ",
            "Ø©": "Ù‡",
            "Ù±": "Ø§",
        }
        for src, dst in replacements.items():
            text = text.replace(src, dst)
        text = re.sub(r"[\u064B-\u065F\u0670\u06D6-\u06ED]", "", text)
        return text

    @staticmethod
    def _keyword_in_text(text, keyword):
        if not text or not keyword:
            return False
        text = WebCrawler._normalize_text(text)
        keyword = WebCrawler._normalize_text(keyword)
        if " " in keyword:
            return keyword in text
        pattern = r"(?<!\\w)" + re.escape(keyword) + r"(?!\\w)"
        return re.search(pattern, text, flags=re.UNICODE) is not None

    def _link_is_relevant(self, link_text, link_url, keywords):
        if not keywords:
            return True
        haystack = f"{link_text} {link_url}"
        return any(self._keyword_in_text(haystack, kw) for kw in keywords)

    @staticmethod
    def _looks_like_listing(url):
        try:
            path = urlparse(url).path or "/"
        except Exception:
            return False
        if path in ["", "/"]:
            return True
        if path.endswith("/"):
            return True
        last_segment = path.rsplit("/", 1)[-1]
        return "." not in last_segment

    def _extract_main_text(self, soup):
        candidates = []
        for tag in ["article", "main"]:
            node = soup.find(tag)
            if node:
                text = node.get_text(separator=" ", strip=True)
                if len(text) >= 200:
                    return text
                candidates.append(text)

        for selector in [
            ".post-content",
            ".article-content",
            ".entry-content",
            ".post",
            ".content",
            "#content",
            ".single-content",
            ".story",
        ]:
            node = soup.select_one(selector)
            if node:
                text = node.get_text(separator=" ", strip=True)
                if len(text) >= 200:
                    return text
                candidates.append(text)

        if candidates:
            return max(candidates, key=len)
        return soup.get_text(separator=" ", strip=True)
    
    def check_robots_txt(self, url):
        """VÃ©rifie robots.txt"""
        if not self.respect_robots_txt:
            return True
        
        try:
            from urllib.robotparser import RobotFileParser
            parsed = urlparse(url)
            robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
            
            cached = self.robots_cache.find_one({'url': robots_url})
            if cached and (datetime.now() - cached['timestamp']).days < 7:
                return cached['allowed']
            
            rp = RobotFileParser()
            rp.set_url(robots_url)
            try:
                rp.read()
                allowed = rp.can_fetch("*", url)
                
                self.robots_cache.update_one(
                    {'url': robots_url},
                    {'$set': {'allowed': allowed, 'timestamp': datetime.now()}},
                    upsert=True
                )
                return allowed
            except:
                return True
        except Exception as e:
            logger.warning(f"Erreur robots.txt: {e}")
            return True
    
    def is_url_recently_crawled(self, url, hours=24):
        """VÃ©rifie si l'URL a Ã©tÃ© crawlÃ©e rÃ©cemment"""
        if not self.mongo_available:
            return False
        recent = self.url_history.find_one({
            'url': url,
            'last_crawled': {'$gte': datetime.now() - timedelta(hours=hours)}
        })
        return recent is not None
    
    def mark_url_crawled(self, url, success=True):
        """Marque une URL comme crawlÃ©e"""
        if not self.mongo_available:
            return
        self.url_history.update_one(
            {'url': url},
            {
                '$set': {
                    'last_crawled': datetime.now(),
                    'success': success
                },
                '$inc': {'crawl_count': 1}
            },
            upsert=True
        )
    
    def add_source(self, url, source_type='website',
                   frequency='daily', schedule_time='09:00',
                   max_hits=100, content_types=None, keywords=None,
                   enabled=True):
        """Ajoute une source"""
        if content_types is None:
            content_types = ['html', 'text']
        if keywords is None:
            keywords = []
        
        source = {
            'url': url,
            'type': source_type,
            'frequency': frequency,
            'schedule_time': schedule_time,
            'max_hits': max_hits,
            'content_types': content_types,
            'keywords': keywords,
            'enabled': enabled,
            'last_crawl': None,
            'status': 'pending',
            'created_at': datetime.now(),
            'failed_attempts': 0,
            'success_count': 0
        }
        
        result = self.sources_collection.insert_one(source)
        logger.info(f"Source ajoutÃ©e: {url}")
        return str(result.inserted_id)
    
    def get_sources(self, enabled_only=False):
        """RÃ©cupÃ¨re les sources"""
        query = {'enabled': True} if enabled_only else {}
        sources = list(self.sources_collection.find(query))
        for source in sources:
            source['_id'] = str(source['_id'])
        return sources
    
    def delete_source(self, source_id):
        """Supprime une source"""
        try:
            from bson.objectid import ObjectId
            self.data_collection.delete_many({'source_id': source_id})
            result = self.sources_collection.delete_one({'_id': ObjectId(source_id)})
            logger.info(f"Source supprimÃ©e: {source_id}")
            return result.deleted_count > 0
        except Exception as e:
            logger.error(f"Erreur suppression: {e}")
            return False
    
    def crawl_url(self, url, content_types, max_hits=100, control=None, stats_cb=None, keywords=None, skip_recent=True, prefer_browser=False):
        """Crawl avec stratÃ©gies anti-blocage avancÃ©es"""
        normalized_types = [ct.lower().strip() for ct in (content_types or [])]
        if "rss" in normalized_types and "xml" not in normalized_types:
            normalized_types.append("xml")
        content_types = normalized_types or ["html"]
        keywords = [k.strip().lower() for k in (keywords or []) if k.strip()]
        keywords = self._expand_keywords(keywords)
        browser_fetcher = None
        first_fetch = True

        def try_browser_fetch(target_url):
            nonlocal browser_fetcher
            if not self.use_browser_fallback:
                return None
            if browser_fetcher is None:
                from crawler.browser_fetcher import BrowserFetcher
                browser_fetcher = BrowserFetcher()
            if stats_cb:
                stats_cb("error", {"url": target_url, "error": "Using browser fallback"})
            return browser_fetcher.fetch(target_url, timeout_sec=self.request_timeout)

        def extract_links(html_bytes, current_url, depth):
            try:
                soup = BeautifulSoup(html_bytes, 'html.parser')
                links_found = 0
                allow_first_hop = depth == 0
                listing_candidates = []
                for link in soup.find_all('a', href=True):
                    absolute_url = urljoin(current_url, link['href'])
                    clean_url = self.anti_blocking.normalize_url(absolute_url)
                    link_text = link.get_text(separator=" ", strip=True)
                    if keywords and not allow_first_hop:
                        if not self._link_is_relevant(link_text, clean_url, keywords):
                            continue
                    elif keywords and allow_first_hop:
                        if self._looks_like_listing(clean_url):
                            listing_candidates.append(clean_url)
                    if self._is_same_domain(url, clean_url):
                        if clean_url not in visited_urls and clean_url not in [f[0] for f in failed_urls]:
                            if clean_url not in [u for u, _ in urls_to_visit]:
                                urls_to_visit.append((clean_url, depth + 1))
                                links_found += 1
                if keywords and allow_first_hop and links_found == 0:
                    for candidate in listing_candidates[:10]:
                        if candidate not in visited_urls and candidate not in [f[0] for f in failed_urls]:
                            if candidate not in [u for u, _ in urls_to_visit]:
                                urls_to_visit.append((candidate, depth + 1))
                                links_found += 1
                if links_found > 0:
                    logger.info(f"   ?+' {links_found} nouveaux liens")
            except Exception:
                pass
        
        def should_stop():
            if control is None:
                return False
            stop_event = getattr(control, "stop_event", None)
            if stop_event is None and isinstance(control, dict):
                stop_event = control.get("stop_event")
            return bool(stop_event and stop_event.is_set())

        def wait_if_paused():
            if control is None:
                return
            pause_event = getattr(control, "pause_event", None)
            if pause_event is None and isinstance(control, dict):
                pause_event = control.get("pause_event")
            if pause_event is not None:
                pause_event.wait()

        if stats_cb:
            stats_cb("start", {"url": url, "max_hits": max_hits})

        collected_data = []
        visited_urls = set()
        urls_to_visit = [(url, 0)]
        failed_urls = {}  # URL -> (retry_count, last_error)
        
        session = self.anti_blocking.create_advanced_session(
            use_proxy=self.use_proxy,
            verify_ssl=self.verify_ssl
        )
        
        domain = urlparse(url).netloc
        last_referer = None
        
        while urls_to_visit and len(collected_data) < max_hits:
            if should_stop():
                if stats_cb:
                    stats_cb("stopped", {"url": url})
                break

            wait_if_paused()

            current_url, depth = urls_to_visit.pop(0)
            normalized_url = self.anti_blocking.normalize_url(current_url)

            if stats_cb:
                stats_cb("attempt", {"url": current_url, "queue": len(urls_to_visit)})
            
            if normalized_url in visited_urls:
                continue
            
            # VÃ©rifier retry count
            if normalized_url in failed_urls:
                retry_count, _ = failed_urls[normalized_url]
                if retry_count >= self.max_retries_per_url:
                    logger.debug(f"AbandonnÃ© aprÃ¨s {retry_count} tentatives: {current_url}")
                    continue
            
            # Robots.txt
            if not self.check_robots_txt(current_url):
                logger.info(f"â›” BloquÃ© par robots.txt: {current_url}")
                failed_urls[normalized_url] = (999, "robots.txt")
                if stats_cb:
                    stats_cb("error", {"url": current_url, "error": "Blocked by robots.txt"})
                continue
            
            # Ã‰viter de re-crawler trop vite
            if skip_recent and self.is_url_recently_crawled(normalized_url, hours=1):
                logger.debug(f"DÃ©jÃ  crawlÃ© rÃ©cemment: {current_url}")
                if stats_cb:
                    stats_cb("error", {"url": current_url, "error": "Recently crawled (1h)"})
                continue
            
            visited_urls.add(normalized_url)
            
            # Optionnel: navigateur en premier sur le tout premier fetch
            if prefer_browser and first_fetch:
                fallback = try_browser_fetch(current_url)
                first_fetch = False
                if fallback:
                    html, final_url, method = fallback
                    data = self._process_html(final_url, html)
                    if data and self._is_relevant(data, keywords):
                        collected_data.append(data)
                        self.mark_url_crawled(normalized_url, success=True)
                        if len(collected_data) < max_hits:
                            extract_links(html, final_url, depth)
                        if stats_cb:
                            stats_cb("success", {"url": current_url, "content_type": "html", "method": method})
                        continue
                    elif data and stats_cb:
                        if len(collected_data) < max_hits:
                            extract_links(html, final_url, depth)
                        stats_cb("error", {"url": current_url, "error": "Filtered by keywords"})

            # Rate limiting adaptatif
            is_retry = normalized_url in failed_urls
            delay = self.anti_blocking.calculate_intelligent_delay(
                self.base_delay, 
                domain, 
                is_retry
            )
            self.rate_limiter.wait_if_needed(domain, delay)
            
            try:
                logger.info(f"ğŸ” Crawl: {current_url}")
                
                # Headers avancÃ©s avec referer intelligent
                headers = self.anti_blocking.get_advanced_headers(
                    url=current_url,
                    referer=last_referer
                )
                
                response = session.get(
                    current_url,
                    headers=headers,
                    timeout=self.request_timeout,
                    allow_redirects=True
                )

                # DÃ©tecter challenge JS mÃªme avec status 200
                if self.use_browser_fallback and self.js_solver.detect_challenge(response):
                    try:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        link_count = len(soup.find_all('a', href=True))
                    except Exception:
                        link_count = 0

                    if link_count < 5:
                        fallback = try_browser_fetch(current_url)
                        if fallback:
                            html, final_url, method = fallback
                            data = self._process_html(final_url, html)
                            if data and self._is_relevant(data, keywords):
                                collected_data.append(data)
                                self.mark_url_crawled(normalized_url, success=True)
                                if len(collected_data) < max_hits:
                                    extract_links(html, final_url, depth)
                                if stats_cb:
                                    stats_cb("success", {"url": current_url, "content_type": "html", "method": method})
                                continue
                            elif data and stats_cb:
                                if len(collected_data) < max_hits:
                                    extract_links(html, final_url, depth)
                                stats_cb("error", {"url": current_url, "error": "Filtered by keywords"})
                
                # Gestion des codes d'erreur
                if response.status_code == 429:
                    logger.warning(f"â±ï¸  429 Rate Limited: {current_url}")
                    self.rate_limiter.report_429(domain)
                    retry_after = int(response.headers.get('Retry-After', 60))
                    if control is not None:
                        retry_after = min(retry_after, 10)
                    logger.info(f"Attente de {retry_after}s...")
                    if stats_cb:
                        stats_cb("error", {"url": current_url, "error": f"Rate limited (retry {retry_after}s)"})
                    time.sleep(retry_after)
                    urls_to_visit.insert(0, (current_url, depth))
                    visited_urls.remove(normalized_url)
                    continue
                
                if response.status_code in [401, 403]:
                    logger.warning(f"ğŸš« {response.status_code} AccÃ¨s refusÃ©: {current_url}")
                    
                    # DÃ©tecter challenge JavaScript
                    if self.js_solver.detect_challenge(response):
                        logger.warning("âš ï¸  Protection anti-bot dÃ©tectÃ©e!")
                        for msg in self.js_solver.suggest_solutions():
                            logger.info(msg)
                    
                    if self.use_browser_fallback:
                        fallback = try_browser_fetch(current_url)
                        if fallback:
                            html, final_url, method = fallback
                            data = self._process_html(final_url, html)
                            if data and self._is_relevant(data, keywords):
                                collected_data.append(data)
                                self.mark_url_crawled(normalized_url, success=True)
                                if len(collected_data) < max_hits:
                                    extract_links(html, final_url, depth)
                                if stats_cb:
                                    stats_cb("success", {"url": current_url, "content_type": "html", "method": method})
                                continue
                            elif data and stats_cb:
                                if len(collected_data) < max_hits:
                                    extract_links(html, final_url, depth)
                                stats_cb("error", {"url": current_url, "error": "Filtered by keywords"})

                    failed_urls[normalized_url] = (
                        failed_urls.get(normalized_url, (0, ""))[0] + 1,
                        f"HTTP {response.status_code}"
                    )
                    if stats_cb:
                        stats_cb("error", {"url": current_url, "error": f"HTTP {response.status_code}"})
                    time.sleep(5)
                    continue
                
                response.raise_for_status()
                
                # Sauvegarder cookies
                self.anti_blocking.save_cookies(session, domain)
                
                # SuccÃ¨s: reporter au rate limiter
                self.rate_limiter.report_success(domain)
                
                # Traiter le contenu
                content_type = response.headers.get('Content-Type', '').lower()
                data = None
                
                data = None

                if 'html' in content_type and 'html' in content_types:
                    data = self._process_html(current_url, response.content)
                    if data:
                        logger.info(f"Fetched: {data['title'][:60]}")
                        
                        # Extraire liens si besoin
                        if len(collected_data) < max_hits:
                            extract_links(response.content, current_url, depth)
                        
                        last_referer = current_url
                
                elif 'xml' in content_type and 'xml' in content_types:
                    data = self._process_xml(current_url, response.content)
                    if data:
                        logger.info(f"Fetched XML: {data['title'][:60]}")
                
                elif 'pdf' in content_type and 'pdf' in content_types:
                    data = self._process_pdf(current_url, response.content)
                    if data:
                        logger.info(f"Fetched PDF: {data['title'][:60]}")
                
                elif 'text' in content_type and 'text' in content_types:
                    data = self._process_text(current_url, response.text)
                    if data:
                        logger.info(f"Fetched text: {data['title'][:60]}")
                
                else:
                    # Essayer HTML par dÃ©faut
                    if 'html' in content_types:
                        data = self._process_html(current_url, response.content)
                        if data:
                            logger.info(f"Fetched page: {data['title'][:60]}")
                
                if data and self._is_relevant(data, keywords):
                    collected_data.append(data)
                    self.mark_url_crawled(normalized_url, success=True)
                    if stats_cb:
                        stats_cb("success", {"url": current_url, "content_type": content_type})
                elif data and stats_cb:
                    stats_cb("error", {"url": current_url, "error": "Filtered by keywords"})
                
            except requests.exceptions.Timeout:
                logger.warning(f"â±ï¸  Timeout: {current_url}")
                if self.use_browser_fallback:
                    fallback = try_browser_fetch(current_url)
                    if fallback:
                        html, final_url, method = fallback
                        data = self._process_html(final_url, html)
                        if data and self._is_relevant(data, keywords):
                            collected_data.append(data)
                            self.mark_url_crawled(normalized_url, success=True)
                            if len(collected_data) < max_hits:
                                extract_links(html, final_url, depth)
                            if stats_cb:
                                stats_cb("success", {"url": current_url, "content_type": "html", "method": method})
                            continue
                        elif data and stats_cb:
                            if len(collected_data) < max_hits:
                                extract_links(html, final_url, depth)
                            stats_cb("error", {"url": current_url, "error": "Filtered by keywords"})
                failed_urls[normalized_url] = (
                    failed_urls.get(normalized_url, (0, ""))[0] + 1,
                    "Timeout"
                )
                if stats_cb:
                    stats_cb("error", {"url": current_url, "error": "Timeout"})
                
            except requests.exceptions.ConnectionError as e:
                logger.warning(f"ğŸ”Œ Erreur connexion: {current_url}")
                if self.use_browser_fallback:
                    fallback = try_browser_fetch(current_url)
                    if fallback:
                        html, final_url, method = fallback
                        data = self._process_html(final_url, html)
                        if data and self._is_relevant(data, keywords):
                            collected_data.append(data)
                            self.mark_url_crawled(normalized_url, success=True)
                            if len(collected_data) < max_hits:
                                extract_links(html, final_url, depth)
                            if stats_cb:
                                stats_cb("success", {"url": current_url, "content_type": "html", "method": method})
                            continue
                        elif data and stats_cb:
                            if len(collected_data) < max_hits:
                                extract_links(html, final_url, depth)
                            stats_cb("error", {"url": current_url, "error": "Filtered by keywords"})
                failed_urls[normalized_url] = (
                    failed_urls.get(normalized_url, (0, ""))[0] + 1,
                    "Connection Error"
                )
                time.sleep(5)
                if stats_cb:
                    stats_cb("error", {"url": current_url, "error": "Connection Error"})
                
            except requests.exceptions.TooManyRedirects:
                logger.warning(f"ğŸ”„ Trop de redirections: {current_url}")
                failed_urls[normalized_url] = (999, "Too Many Redirects")
                if stats_cb:
                    stats_cb("error", {"url": current_url, "error": "Too Many Redirects"})
                
            except Exception as e:
                logger.warning(f"âŒ Erreur: {current_url} - {str(e)[:100]}")
                failed_urls[normalized_url] = (
                    failed_urls.get(normalized_url, (0, ""))[0] + 1,
                    str(e)[:100]
                )
                if stats_cb:
                    stats_cb("error", {"url": current_url, "error": str(e)[:100]})
        
        session.close()
        logger.info(f"ğŸ“Š RÃ©sumÃ©: {len(collected_data)} pages collectÃ©es, {len(failed_urls)} Ã©checs")

        if stats_cb:
            stats_cb("done", {"collected": len(collected_data), "failed": len(failed_urls)})
        
        return collected_data
    
    def _is_same_domain(self, base_url, check_url):
        """VÃ©rifie si mÃªme domaine"""
        base = urlparse(base_url).netloc.lower()
        check = urlparse(check_url).netloc.lower()
        base = base.replace("www.", "")
        check = check.replace("www.", "")
        if base == check:
            return True
        return check.endswith("." + base) or base.endswith("." + check)
    
    def _process_html(self, url, content):
        """Traite HTML"""
        try:
            soup = BeautifulSoup(content, 'html.parser')
            
            for script in soup(['script', 'style', 'nav', 'footer', 'aside', 'header']):
                script.decompose()
            
            title = soup.title.string if soup.title else 'Sans titre'
            title = title.strip()[:200]
            
            text_content = self._extract_main_text(soup)
            
            keywords = []
            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
            if meta_keywords and meta_keywords.get('content'):
                keywords = [k.strip() for k in meta_keywords['content'].split(',')][:10]
            
            description = ''
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                description = meta_desc['content'][:500]
            
            return {
                'url': url,
                'title': title,
                'description': description,
                'content': text_content[:10000],
                'content_type': 'html',
                'keywords': keywords,
                'timestamp': datetime.now()
            }
        except Exception as e:
            logger.error(f"Erreur HTML: {e}")
            return None
    
    def _process_xml(self, url, content):
        """Traite XML/RSS"""
        try:
            soup = BeautifulSoup(content, 'xml')
            items = soup.find_all('item')
            if items:
                item = items[0]
                title = item.find('title').text if item.find('title') else 'Sans titre'
                description = item.find('description').text if item.find('description') else ''
                
                return {
                    'url': url,
                    'title': title,
                    'description': description[:500],
                    'content': description[:5000],
                    'content_type': 'xml',
                    'keywords': [],
                    'timestamp': datetime.now()
                }
            return None
        except Exception as e:
            logger.error(f"Erreur XML: {e}")
            return None
    
    def _process_pdf(self, url, content):
        """Traite PDF"""
        try:
            pdf_file = io.BytesIO(content)
            text_content = ""
            
            with pdfplumber.open(pdf_file) as pdf:
                for page in pdf.pages[:10]:
                    text_content += page.extract_text() or ""
            
            return {
                'url': url,
                'title': url.split('/')[-1],
                'description': text_content[:500],
                'content': text_content[:10000],
                'content_type': 'pdf',
                'keywords': [],
                'timestamp': datetime.now()
            }
        except Exception as e:
            logger.error(f"Erreur PDF: {e}")
            return None
    
    def _process_text(self, url, content):
        """Traite texte brut"""
        try:
            return {
                'url': url,
                'title': url.split('/')[-1],
                'description': content[:500],
                'content': content[:10000],
                'content_type': 'text',
                'keywords': [],
                'timestamp': datetime.now()
            }
        except Exception as e:
            logger.error(f"Erreur texte: {e}")
            return None

    def _is_relevant(self, data, keywords):
        if not keywords:
            return True

        title = str(data.get('title', ''))
        description = str(data.get('description', ''))
        content = str(data.get('content', ''))
        url = str(data.get('url', ''))
        meta_keywords = " ".join(data.get('keywords', []) or [])

        strict_finance = "finance" in keywords
        strict_health = "health" in keywords
        strict_mode = strict_finance or strict_health

        finance_terms = {
            "finance", "financial", "economy", "economic", "bank", "banks", "banking",
            "investment", "investments", "stock", "stocks", "market", "markets",
            "bond", "bonds", "inflation", "budget", "tax", "taxes", "loan", "loans",
            "credit", "currency", "currencies", "fund", "funds", "finance ministry",
            "ministry of finance", "Ã©conomie", "Ã©conomique", "banque", "banques",
            "bourse", "marchÃ©", "marchÃ©s", "investissement", "investissements",
            "impÃ´t", "impÃ´ts", "crÃ©dit", "monnaie", "finances", "ØªÙ…ÙˆÙŠÙ„", "Ù…Ø§Ù„ÙŠ",
            "Ù…Ø§Ù„ÙŠØ©", "Ø§Ù‚ØªØµØ§Ø¯", "Ø§Ù‚ØªØµØ§Ø¯ÙŠ", "Ø¨Ù†Ùƒ", "Ø¨Ù†ÙˆÙƒ", "Ø§Ø³ØªØ«Ù…Ø§Ø±", "Ø§Ø³ØªØ«Ù…Ø§Ø±Ø§Øª",
            "Ø¨ÙˆØ±ØµØ©", "Ø³ÙˆÙ‚", "Ø£Ø³ÙˆØ§Ù‚", "ØªØ¶Ø®Ù…", "Ù…ÙŠØ²Ø§Ù†ÙŠØ©", "Ø¶Ø±Ø§Ø¦Ø¨", "Ù‚Ø±Ø¶", "Ù‚Ø±ÙˆØ¶",
            "ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©",
        }
        health_terms = {
            "health", "healthcare", "medical", "medicine", "doctor", "doctors",
            "hospital", "hospitals", "clinic", "clinics", "patient", "patients",
            "public health", "vaccine", "vaccines", "epidemic", "pandemic",
            "disease", "diseases", "treatment", "pharmacy", "pharmacies",
            "ministry of health", "santÃ©", "sanitaire", "mÃ©dical", "mÃ©decine",
            "hÃ´pital", "hÃ´pitaux", "clinique", "cliniques", "vaccin", "vaccins",
            "Ã©pidÃ©mie", "pandÃ©mie", "maladie", "maladies", "traitement",
            "pharmacie", "pharmacies", "ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø©", "ØµØ­Ø©", "ØµØ­ÙŠ", "Ø·Ø¨ÙŠØ¨",
            "Ø£Ø·Ø¨Ø§Ø¡", "Ù…Ø³ØªØ´ÙÙ‰", "Ù…Ø³ØªØ´ÙÙŠØ§Øª", "Ø¹ÙŠØ§Ø¯Ø©", "Ø¹ÙŠØ§Ø¯Ø§Øª", "Ù…Ø±ÙŠØ¶", "Ù…Ø±Ø¶Ù‰",
            "Ù„Ù‚Ø§Ø­", "Ù„Ù‚Ø§Ø­Ø§Øª", "ÙˆØ¨Ø§Ø¡", "Ø¬Ø§Ø¦Ø­Ø©", "Ù…Ø±Ø¶", "Ø£Ù…Ø±Ø§Ø¶", "Ø¹Ù„Ø§Ø¬",
            "ØµÙŠØ¯Ù„ÙŠØ©", "ØµÙŠØ¯Ù„ÙŠØ§Øª",
        }
        precision_terms = set()
        if strict_finance:
            precision_terms.update(finance_terms)
        if strict_health:
            precision_terms.update(health_terms)

        title_match = any(self._keyword_in_text(title, kw) for kw in keywords)
        description_match = any(self._keyword_in_text(description, kw) for kw in keywords)
        url_match = any(self._keyword_in_text(url, kw) for kw in keywords)
        meta_match = any(self._keyword_in_text(meta_keywords, kw) for kw in keywords)

        if title_match or description_match or url_match or meta_match:
            if not strict_mode:
                return True
            high_precision = any(
                self._keyword_in_text(title, kw)
                or self._keyword_in_text(description, kw)
                or self._keyword_in_text(url, kw)
                or self._keyword_in_text(meta_keywords, kw)
                for kw in precision_terms
            )
            if high_precision:
                return True
        elif strict_mode:
            high_precision = any(
                self._keyword_in_text(title, kw)
                or self._keyword_in_text(description, kw)
                or self._keyword_in_text(url, kw)
                or self._keyword_in_text(meta_keywords, kw)
                for kw in precision_terms
            )
            if not high_precision:
                return False

        if self._looks_like_listing(url):
            return False

        normalized_content = self._normalize_text(content)
        if len(normalized_content) < 300:
            return False

        content_matches = {kw for kw in keywords if self._keyword_in_text(normalized_content, kw)}
        if not content_matches:
            return False

        if strict_mode:
            precision_hits = {kw for kw in precision_terms if self._keyword_in_text(normalized_content, kw)}
            return len(content_matches) >= 2 and len(precision_hits) >= 1

        content_hits = 0
        for kw in content_matches:
            if " " in kw:
                if kw in normalized_content:
                    content_hits += 2
            else:
                matches = re.findall(r"(?<!\\w)" + re.escape(kw) + r"(?!\\w)", normalized_content, flags=re.UNICODE)
                content_hits += min(len(matches), 3)

        return content_hits >= 3
    
    def crawl_source(self, source_id):
        """Crawl une source"""
        try:
            from bson.objectid import ObjectId
            source = self.sources_collection.find_one({'_id': ObjectId(source_id)})
            
            if not source or not source.get('enabled'):
                logger.warning(f"Source {source_id} introuvable ou dÃ©sactivÃ©e")
                return 0
            
            logger.info(f"ğŸš€ DÃ©but crawl: {source['url']}")
            
            self.sources_collection.update_one(
                {'_id': ObjectId(source_id)},
                {'$set': {'status': 'crawling'}}
            )
            
            collected_data = self.crawl_url(
                source['url'],
                source['content_types'],
                source['max_hits'],
                keywords=source.get('keywords', [])
            )
            
            count = 0
            for data in collected_data:
                data['source_id'] = source_id
                try:
                    self.data_collection.insert_one(data)
                    count += 1
                except pymongo.errors.DuplicateKeyError:
                    logger.debug(f"Doublon ignorÃ©: {data['url']}")
            
            self.sources_collection.update_one(
                {'_id': ObjectId(source_id)},
                {
                    '$set': {
                        'status': 'completed',
                        'last_crawl': datetime.now(),
                        'failed_attempts': 0
                    },
                    '$inc': {'success_count': 1}
                }
            )
            
            logger.info(f"âœ… Crawl terminÃ©: {count} Ã©lÃ©ments sauvegardÃ©s")
            return count
            
        except Exception as e:
            logger.error(f"âŒ Erreur crawl: {e}")
            
            try:
                from bson.objectid import ObjectId
                self.sources_collection.update_one(
                    {'_id': ObjectId(source_id)},
                    {
                        '$set': {'status': 'failed'},
                        '$inc': {'failed_attempts': 1}
                    }
                )
                
                source = self.sources_collection.find_one({'_id': ObjectId(source_id)})
                if source and source.get('failed_attempts', 0) >= 5:
                    self.sources_collection.update_one(
                        {'_id': ObjectId(source_id)},
                        {'$set': {'enabled': False, 'status': 'disabled_after_failures'}}
                    )
                    logger.warning(f"âš ï¸  Source {source_id} dÃ©sactivÃ©e aprÃ¨s 5 Ã©checs")
            except:
                pass
            
            return 0
    
    def search_data(self, query, limit=50):
        """Recherche par mots-clÃ©s"""
        try:
            results = list(self.data_collection.find(
                {'$text': {'$search': query}},
                {'score': {'$meta': 'textScore'}}
            ).sort([('score', {'$meta': 'textScore'})]).limit(limit))
            
            for result in results:
                result['_id'] = str(result['_id'])
            
            logger.info(f"ğŸ” Recherche '{query}': {len(results)} rÃ©sultats")
            return results
            
        except Exception as e:
            logger.error(f"Erreur recherche: {e}")
            return []
    
    def get_statistics(self):
        """Statistiques"""
        return {
            'total_sources': self.sources_collection.count_documents({}),
            'active_sources': self.sources_collection.count_documents({'enabled': True}),
            'failed_sources': self.sources_collection.count_documents({'status': 'failed'}),
            'total_data': self.data_collection.count_documents({}),
            'urls_crawled': self.url_history.count_documents({}),
            'last_update': datetime.now()
        }
    
    def schedule_crawls(self):
        """Planificateur"""
        sources = self.get_sources(enabled_only=True)
        
        for source in sources:
            source_id = source['_id']
            frequency = source['frequency']
            schedule_time = source.get('schedule_time', '09:00')
            
            if frequency == 'hourly':
                schedule.every().hour.do(self.crawl_source, source_id)
            elif frequency == 'daily':
                schedule.every().day.at(schedule_time).do(self.crawl_source, source_id)
            elif frequency == 'weekly':
                schedule.every().week.at(schedule_time).do(self.crawl_source, source_id)
            elif frequency == 'monthly':
                schedule.every(30).days.at(schedule_time).do(self.crawl_source, source_id)
        
        logger.info(f"â° Planificateur: {len(sources)} sources")
        
        def run_scheduler():
            while True:
                schedule.run_pending()
                time.sleep(60)
        
        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()
        logger.info("âœ“ Planificateur dÃ©marrÃ©")
    
    def close(self):
        """Ferme MongoDB"""
        self.client.close()
        logger.info("âœ“ Connexion fermÃ©e")


def main():
    """Interface console"""
    print("=" * 70)
    print("    WEB CRAWLER AVANCÃ‰ - STRATÃ‰GIES ANTI-BLOCAGE    ")
    print("=" * 70)
    print("\nğŸ¯ Nouvelles fonctionnalitÃ©s anti-blocage:")
    print("  â€¢ Rate limiting adaptatif (apprend des rÃ©ponses)")
    print("  â€¢ Headers avancÃ©s avec fingerprinting")
    print("  â€¢ Gestion intelligente des cookies")
    print("  â€¢ Normalisation d'URLs anti-doublons")
    print("  â€¢ DÃ©tection de challenges JavaScript")
    print("  â€¢ Retry intelligent avec backoff")
    print("  â€¢ Historique d'URLs pour Ã©viter re-crawl")
    print("\nâš™ï¸  Configuration:")
    
    use_proxy = input("Proxies? (o/n) [n]: ").strip().lower() == 'o'
    
    if use_proxy and not AdvancedAntiBlockingStrategy.PROXIES:
        print("\nâš ï¸  Aucun proxy configurÃ©!")
        if input("Continuer sans proxy? (o/n): ").strip().lower() != 'o':
            use_proxy = False
    
    base_delay = float(input("DÃ©lai de base (secondes) [2]: ").strip() or '2')
    respect_robots = input("Respecter robots.txt? (o/n) [o]: ").strip().lower() != 'n'
    verify_ssl = input("VÃ©rifier SSL? (o/n) [o]: ").strip().lower() != 'n'
    max_retries = int(input("Max tentatives par URL [3]: ").strip() or '3')
    
    crawler = WebCrawler(
        use_proxy=use_proxy,
        base_delay=base_delay,
        respect_robots_txt=respect_robots,
        verify_ssl=verify_ssl,
        max_retries_per_url=max_retries
    )
    
    while True:
        print("\n" + "="*70)
        print("MENU PRINCIPAL")
        print("="*70)
        print("1. â• Ajouter une source")
        print("2. ğŸ“‹ Lister les sources")
        print("3. ğŸ” Crawler une source")
        print("4. ğŸš€ Crawler toutes les sources actives")
        print("5. ğŸ” Rechercher dans les donnÃ©es")
        print("6. ğŸ“Š Statistiques")
        print("7. ğŸ—‘ï¸  Supprimer une source")
        print("8. â° DÃ©marrer le planificateur")
        print("9. ğŸšª Quitter")
        
        choice = input("\nChoix: ").strip()
        
        if choice == '1':
            print("\n" + "="*70)
            print("AJOUTER UNE SOURCE")
            print("="*70)
            print("\nğŸ’¡ Sites recommandÃ©s pour tester:")
            print("  â€¢ http://books.toscrape.com/")
            print("  â€¢ https://news.ycombinator.com/")
            print("  â€¢ https://en.wikipedia.org/wiki/Web_scraping")
            print()
            
            url = input("URL: ").strip()
            source_type = input("Type [website]: ").strip() or 'website'
            frequency = input("FrÃ©quence (hourly/daily/weekly/monthly) [daily]: ").strip() or 'daily'
            schedule_time = input("Heure (HH:MM) [09:00]: ").strip() or '09:00'
            max_hits = int(input("Max pages [100]: ").strip() or '100')
            content_types_input = input("Types (html,xml,rss,pdf,text) [html]: ").strip() or 'html'
            content_types = [ct.strip() for ct in content_types_input.split(',')]
            keywords_input = input("Mots-cles (finance, education, ... ) [vide]: ").strip()
            keywords = [kw.strip() for kw in keywords_input.split(',') if kw.strip()]
            
            source_id = crawler.add_source(
                url=url,
                source_type=source_type,
                frequency=frequency,
                schedule_time=schedule_time,
                max_hits=max_hits,
                content_types=content_types,
                keywords=keywords
            )
            print(f"\nâœ… Source ajoutÃ©e! ID: {source_id}")
        
        elif choice == '2':
            sources = crawler.get_sources()
            print(f"\n" + "="*70)
            print(f"SOURCES ({len(sources)})")
            print("="*70)
            for i, source in enumerate(sources, 1):
                print(f"\n{i}. ğŸ†” {source['_id']}")
                print(f"   ğŸŒ URL: {source['url']}")
                print(f"   ğŸ“ Type: {source['type']}")
                print(f"   â° FrÃ©quence: {source['frequency']}")
                print(f"   âœ… Actif: {'Oui' if source['enabled'] else 'Non'}")
                print(f"   ğŸ“Š Statut: {source.get('status', 'N/A')}")
                print(f"   âŒ Ã‰checs: {source.get('failed_attempts', 0)}")
                print(f"   âœ”ï¸  SuccÃ¨s: {source.get('success_count', 0)}")
                print(f"   ğŸ• Dernier crawl: {source.get('last_crawl', 'Jamais')}")
        
        elif choice == '3':
            source_id = input("\nğŸ†” ID de la source: ").strip()
            count = crawler.crawl_source(source_id)
            print(f"\nâœ… {count} Ã©lÃ©ments collectÃ©s")
        
        elif choice == '4':
            sources = crawler.get_sources(enabled_only=True)
            print(f"\nğŸš€ Crawl de {len(sources)} sources...")
            total = 0
            for source in sources:
                count = crawler.crawl_source(source['_id'])
                total += count
            print(f"\nâœ… Total: {total} Ã©lÃ©ments")
        
        elif choice == '5':
            query = input("\nğŸ” Recherche: ").strip()
            results = crawler.search_data(query)
            print(f"\n" + "="*70)
            print(f"RÃ‰SULTATS ({len(results)})")
            print("="*70)
            for i, result in enumerate(results[:10], 1):
                print(f"\n{i}. ğŸ“„ {result['title']}")
                print(f"   ğŸŒ {result['url']}")
                print(f"   ğŸ“ Type: {result['content_type']}")
                if result.get('description'):
                    print(f"   ğŸ“ {result['description'][:150]}...")
        
        elif choice == '6':
            stats = crawler.get_statistics()
            print(f"\n" + "="*70)
            print("STATISTIQUES")
            print("="*70)
            print(f"ğŸ“¦ Total sources: {stats['total_sources']}")
            print(f"âœ… Sources actives: {stats['active_sources']}")
            print(f"âŒ Sources en Ã©chec: {stats['failed_sources']}")
            print(f"ğŸ“„ Total donnÃ©es: {stats['total_data']}")
            print(f"ğŸ”— URLs crawlÃ©es: {stats['urls_crawled']}")
        
        elif choice == '7':
            source_id = input("\nğŸ—‘ï¸  ID Ã  supprimer: ").strip()
            if crawler.delete_source(source_id):
                print("\nâœ… Source supprimÃ©e")
            else:
                print("\nâŒ Erreur")
        
        elif choice == '8':
            print("\nâ° DÃ©marrage du planificateur...")
            crawler.schedule_crawls()
            print("âœ… Planificateur actif (Ctrl+C pour arrÃªter)")
            try:
                while True:
                    time.sleep(1)
            except KeyboardInterrupt:
                print("\n\nâ¹ï¸  ArrÃªtÃ©")
        
        elif choice == '9':
            crawler.close()
            print("\nğŸ‘‹ Au revoir!")
            break


if __name__ == "__main__":
    main()
