from crawler.web_crawler import WebCrawler
from preprocessing.cleaner import clean_text, truncate_text
from llm.extractor import extract_knowledge
from graph.builder import GraphBuilder
from visualization.plotter import visualize_graph

def pipeline(url: str, max_pages: int = 5):
    """Pipeline complet : Crawl â†’ LLM (Groq) â†’ Graph â†’ Viz"""
    print("\n" + "="*60)
    print("ğŸš€ GRAPHCRAWLER - Pipeline avec Groq")
    print("="*60)
    print(f"\nğŸ” URL cible: {url}")
    print(f"ğŸ“„ Pages max: {max_pages}\n")
    
    # Initialiser le builder UNE SEULE FOIS
    builder = GraphBuilder()
    
    # 1. Crawl
    print("â³ Ã‰tape 1/4 : Crawling en cours...")
    crawler = WebCrawler()
    
    try:
        data = crawler.crawl_url(url, content_types=['html'], max_hits=max_pages)
    except Exception as e:
        print(f"âŒ Erreur crawl: {e}")
        crawler.close()
        builder.close()
        return
    
    if not data:
        print("âŒ Aucune donnÃ©e crawlÃ©e")
        crawler.close()
        builder.close()
        return
    
    print(f"âœ… {len(data)} pages crawlÃ©es avec succÃ¨s\n")
    
    # 2. Traiter chaque page
    print("â³ Ã‰tape 2/4 : Extraction avec Groq...")
    total_entities = 0
    total_relations = 0
    all_graphs = []  # â† Stocker les graphes en mÃ©moire aussi
    
    for i, item in enumerate(data, 1):
        print(f"\nğŸ“„ [{i}/{len(data)}] {item['title'][:50]}...")
        
        # Nettoyer
        text = clean_text(item['content'])
        text = truncate_text(text, max_chars=6000)
        
        if len(text) < 100:
            print("   âš ï¸  Texte trop court, ignorÃ©")
            continue
        
        # LLM
        print("   ğŸ¤– Analyse par Groq...")
        knowledge = extract_knowledge(text)
        
        entities_count = len(knowledge.get('entities', []))
        relations_count = len(knowledge.get('relations', []))
        
        total_entities += entities_count
        total_relations += relations_count
        
        # Graph
        if entities_count > 0:
            graph = builder.build_graph(knowledge, item['url'])
            print(f"   ğŸ“Š Graphe gÃ©nÃ©rÃ© : {len(graph.nodes)} nÅ“uds, {len(graph.edges)} liens")
            
            graph_id = builder.save_graph(graph)
            if graph_id:
                print(f"   ğŸ’¾ Graphe sauvegardÃ© (ID: {graph_id[:8]}...)")
                all_graphs.append(graph)  # â† Garder en mÃ©moire
            else:
                print("   âš ï¸  Ã‰chec sauvegarde")
        else:
            print("   âš ï¸  Aucune entitÃ© extraite")
    
    print(f"\nâœ… Extraction terminÃ©e:")
    print(f"   ğŸ“Š Total entitÃ©s: {total_entities}")
    print(f"   ğŸ”— Total relations: {total_relations}\n")
    
    # 3. RÃ©cupÃ©rer les graphes
    print("â³ Ã‰tape 3/4 : Construction du graphe global...")
    
    if not all_graphs:
        print("âŒ Aucun graphe Ã  visualiser")
        crawler.close()
        builder.close()
        return
    
    print(f"âœ… {len(all_graphs)} graphe(s) construit(s)\n")
    
    # 4. Visualiser le dernier graphe OU fusionner tous les graphes
    print("â³ Ã‰tape 4/4 : GÃ©nÃ©ration de la visualisation...")
    
    try:
        # Option 1: Visualiser le dernier graphe
        last_graph = all_graphs[-1]
        
        # Convertir en format dict pour le plotter
        graph_dict = {
            'nodes': [{'name': n.name, 'type': n.type} for n in last_graph.nodes],
            'edges': [{'source': e.source, 'target': e.target, 'type': e.type} for e in last_graph.edges],
            'source_url': last_graph.source_url
        }
        
        visualize_graph(graph_dict, "output_graph.png")
        
        # Option 2: Fusionner tous les graphes (dÃ©commentez si vous voulez)
        # from graph.models import Graph
        # merged = Graph(nodes=[], edges=[], source_url="Merged", created_at=datetime.now())
        # for g in all_graphs:
        #     merged.merge(g)
        # 
        # merged_dict = {
        #     'nodes': [{'name': n.name, 'type': n.type} for n in merged.nodes],
        #     'edges': [{'source': e.source, 'target': e.target, 'type': e.type} for e in merged.edges],
        #     'source_url': "Graphe fusionnÃ©"
        # }
        # visualize_graph(merged_dict, "merged_graph.png")
        
    except Exception as e:
        print(f"âŒ Erreur visualisation: {e}")
        import traceback
        traceback.print_exc()
    
    # Nettoyage
    crawler.close()
    builder.close()
    
    print("\n" + "="*60)
    print("ğŸ‰ Pipeline terminÃ© avec succÃ¨s!")
    print("="*60)
    print(f"\nğŸ“Š RÃ©sumÃ©:")
    print(f"   â€¢ Pages analysÃ©es: {len(data)}")
    print(f"   â€¢ EntitÃ©s totales: {total_entities}")
    print(f"   â€¢ Relations totales: {total_relations}")
    print(f"   â€¢ Graphes crÃ©Ã©s: {len(all_graphs)}")
    print(f"   â€¢ Fichier de sortie: output_graph.png\n")

if __name__ == "__main__":
    print("\nğŸ•·ï¸  GRAPHCRAWLER - Powered by Groq\n")
    
    url = input("ğŸŒ Entrez l'URL Ã  crawler: ").strip()
    
    if not url:
        print("âŒ URL invalide")
        exit(1)
    
    # Valider l'URL
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    # Demander le nombre de pages
    try:
        max_pages = input("ğŸ“„ Nombre de pages max [5]: ").strip()
        max_pages = int(max_pages) if max_pages else 5
    except:
        max_pages = 5
    
    # Lancer le pipeline
    pipeline(url, max_pages)